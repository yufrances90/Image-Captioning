{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from utils.data_loader import get_loader\n",
    "from utils.model import EncoderCNN, DecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select appropriate values for the Python variables below.\n",
    "batch_size = 32          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 100             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'logs/training_log_1.txt'       # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_value = 2             # the maximum gradient value for clipping\n",
    "num_layers = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.65s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395db640f22e4cc19903a3f1b6626b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=414113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained weights.\n",
    "encoder.load_state_dict(torch.load('./models/encoder-2.pkl'))\n",
    "decoder.load_state_dict(torch.load('./models/decoder-2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embedding): Embedding(9955, 512)\n",
       "  (lstm): LSTM(512, 512, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=9955, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(log_file, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [100/12942], Loss: 2.2045, Perplexity: 9.0654\n",
      "Epoch [2/3], Step [200/12942], Loss: 2.5887, Perplexity: 13.3130\n",
      "Epoch [2/3], Step [300/12942], Loss: 2.2018, Perplexity: 9.04163\n",
      "Epoch [2/3], Step [400/12942], Loss: 2.5436, Perplexity: 12.7249\n",
      "Epoch [2/3], Step [500/12942], Loss: 2.3960, Perplexity: 10.9791\n",
      "Epoch [2/3], Step [600/12942], Loss: 2.5241, Perplexity: 12.4801\n",
      "Epoch [2/3], Step [700/12942], Loss: 2.3056, Perplexity: 10.0305\n",
      "Epoch [2/3], Step [800/12942], Loss: 2.5360, Perplexity: 12.6296\n",
      "Epoch [2/3], Step [900/12942], Loss: 2.1774, Perplexity: 8.82310\n",
      "Epoch [2/3], Step [1000/12942], Loss: 2.3816, Perplexity: 10.8219\n",
      "Epoch [2/3], Step [1100/12942], Loss: 2.4477, Perplexity: 11.5615\n",
      "Epoch [2/3], Step [1200/12942], Loss: 2.5823, Perplexity: 13.2279\n",
      "Epoch [2/3], Step [1300/12942], Loss: 2.2933, Perplexity: 9.90778\n",
      "Epoch [2/3], Step [1400/12942], Loss: 2.3841, Perplexity: 10.8488\n",
      "Epoch [2/3], Step [1500/12942], Loss: 2.4668, Perplexity: 11.7844\n",
      "Epoch [2/3], Step [1600/12942], Loss: 2.3263, Perplexity: 10.2397\n",
      "Epoch [2/3], Step [1700/12942], Loss: 2.4816, Perplexity: 11.9601\n",
      "Epoch [2/3], Step [1800/12942], Loss: 2.7498, Perplexity: 15.6391\n",
      "Epoch [2/3], Step [1900/12942], Loss: 2.4189, Perplexity: 11.2340\n",
      "Epoch [2/3], Step [2000/12942], Loss: 2.2916, Perplexity: 9.89032\n",
      "Epoch [2/3], Step [2100/12942], Loss: 2.5123, Perplexity: 12.3329\n",
      "Epoch [2/3], Step [2200/12942], Loss: 1.9700, Perplexity: 7.17068\n",
      "Epoch [2/3], Step [2300/12942], Loss: 2.2108, Perplexity: 9.12314\n",
      "Epoch [2/3], Step [2400/12942], Loss: 2.1420, Perplexity: 8.51684\n",
      "Epoch [2/3], Step [2500/12942], Loss: 2.3369, Perplexity: 10.3489\n",
      "Epoch [2/3], Step [2600/12942], Loss: 2.3973, Perplexity: 10.9937\n",
      "Epoch [2/3], Step [2700/12942], Loss: 3.0554, Perplexity: 21.2301\n",
      "Epoch [2/3], Step [2800/12942], Loss: 1.9654, Perplexity: 7.13763\n",
      "Epoch [2/3], Step [2900/12942], Loss: 2.9124, Perplexity: 18.4012\n",
      "Epoch [2/3], Step [3000/12942], Loss: 2.4147, Perplexity: 11.1864\n",
      "Epoch [2/3], Step [3100/12942], Loss: 2.3951, Perplexity: 10.9691\n",
      "Epoch [2/3], Step [3200/12942], Loss: 2.0464, Perplexity: 7.74030\n",
      "Epoch [2/3], Step [3300/12942], Loss: 2.1455, Perplexity: 8.54662\n",
      "Epoch [2/3], Step [3400/12942], Loss: 2.4746, Perplexity: 11.8764\n",
      "Epoch [2/3], Step [3500/12942], Loss: 2.0806, Perplexity: 8.00932\n",
      "Epoch [2/3], Step [3600/12942], Loss: 2.3490, Perplexity: 10.4753\n",
      "Epoch [2/3], Step [3700/12942], Loss: 2.3670, Perplexity: 10.6658\n",
      "Epoch [2/3], Step [3800/12942], Loss: 2.6084, Perplexity: 13.5779\n",
      "Epoch [2/3], Step [3900/12942], Loss: 2.3694, Perplexity: 10.6906\n",
      "Epoch [2/3], Step [4000/12942], Loss: 3.1521, Perplexity: 23.3845\n",
      "Epoch [2/3], Step [4100/12942], Loss: 2.3876, Perplexity: 10.8873\n",
      "Epoch [2/3], Step [4200/12942], Loss: 2.2518, Perplexity: 9.50509\n",
      "Epoch [2/3], Step [4300/12942], Loss: 2.3169, Perplexity: 10.1443\n",
      "Epoch [2/3], Step [4400/12942], Loss: 2.3565, Perplexity: 10.5543\n",
      "Epoch [2/3], Step [4500/12942], Loss: 2.4777, Perplexity: 11.9138\n",
      "Epoch [2/3], Step [4600/12942], Loss: 2.3505, Perplexity: 10.4912\n",
      "Epoch [2/3], Step [4700/12942], Loss: 2.2086, Perplexity: 9.10279\n",
      "Epoch [2/3], Step [4800/12942], Loss: 2.0108, Perplexity: 7.46915\n",
      "Epoch [2/3], Step [4900/12942], Loss: 1.9302, Perplexity: 6.89080\n",
      "Epoch [2/3], Step [5000/12942], Loss: 1.9528, Perplexity: 7.04861\n",
      "Epoch [2/3], Step [5100/12942], Loss: 2.2597, Perplexity: 9.58018\n",
      "Epoch [2/3], Step [5200/12942], Loss: 2.4736, Perplexity: 11.8653\n",
      "Epoch [2/3], Step [5300/12942], Loss: 2.3786, Perplexity: 10.7902\n",
      "Epoch [2/3], Step [5400/12942], Loss: 2.1791, Perplexity: 8.83805\n",
      "Epoch [2/3], Step [5500/12942], Loss: 3.0930, Perplexity: 22.0440\n",
      "Epoch [2/3], Step [5600/12942], Loss: 2.3173, Perplexity: 10.1480\n",
      "Epoch [2/3], Step [5700/12942], Loss: 2.1082, Perplexity: 8.23387\n",
      "Epoch [2/3], Step [5800/12942], Loss: 2.4369, Perplexity: 11.4376\n",
      "Epoch [2/3], Step [5900/12942], Loss: 2.3301, Perplexity: 10.2786\n",
      "Epoch [2/3], Step [6000/12942], Loss: 2.2036, Perplexity: 9.05753\n",
      "Epoch [2/3], Step [6100/12942], Loss: 2.4433, Perplexity: 11.5105\n",
      "Epoch [2/3], Step [6200/12942], Loss: 2.3130, Perplexity: 10.1049\n",
      "Epoch [2/3], Step [6300/12942], Loss: 2.1697, Perplexity: 8.75521\n",
      "Epoch [2/3], Step [6400/12942], Loss: 2.1242, Perplexity: 8.36657\n",
      "Epoch [2/3], Step [6500/12942], Loss: 2.4105, Perplexity: 11.1398\n",
      "Epoch [2/3], Step [6600/12942], Loss: 1.8602, Perplexity: 6.42479\n",
      "Epoch [2/3], Step [6700/12942], Loss: 2.2774, Perplexity: 9.75173\n",
      "Epoch [2/3], Step [6800/12942], Loss: 2.1800, Perplexity: 8.846371\n",
      "Epoch [2/3], Step [6900/12942], Loss: 2.8217, Perplexity: 16.8046\n",
      "Epoch [2/3], Step [7000/12942], Loss: 2.2240, Perplexity: 9.24416\n",
      "Epoch [2/3], Step [7100/12942], Loss: 2.2763, Perplexity: 9.74091\n",
      "Epoch [2/3], Step [7200/12942], Loss: 2.2594, Perplexity: 9.57726\n",
      "Epoch [2/3], Step [7300/12942], Loss: 2.0007, Perplexity: 7.39409\n",
      "Epoch [2/3], Step [7400/12942], Loss: 2.1277, Perplexity: 8.39590\n",
      "Epoch [2/3], Step [7500/12942], Loss: 2.1226, Perplexity: 8.35319\n",
      "Epoch [2/3], Step [7600/12942], Loss: 2.1274, Perplexity: 8.39272\n",
      "Epoch [2/3], Step [7700/12942], Loss: 2.2866, Perplexity: 9.84189\n",
      "Epoch [2/3], Step [7800/12942], Loss: 2.3160, Perplexity: 10.1351\n",
      "Epoch [2/3], Step [7900/12942], Loss: 2.0055, Perplexity: 7.42963\n",
      "Epoch [2/3], Step [8000/12942], Loss: 2.4388, Perplexity: 11.4590\n",
      "Epoch [2/3], Step [8100/12942], Loss: 2.2507, Perplexity: 9.49420\n",
      "Epoch [2/3], Step [8200/12942], Loss: 2.0623, Perplexity: 7.86394\n",
      "Epoch [2/3], Step [8300/12942], Loss: 2.8385, Perplexity: 17.0899\n",
      "Epoch [2/3], Step [8400/12942], Loss: 2.1918, Perplexity: 8.95097\n",
      "Epoch [2/3], Step [8500/12942], Loss: 2.2744, Perplexity: 9.72231\n",
      "Epoch [2/3], Step [8600/12942], Loss: 2.5037, Perplexity: 12.2274\n",
      "Epoch [2/3], Step [8700/12942], Loss: 2.5717, Perplexity: 13.0881\n",
      "Epoch [2/3], Step [8800/12942], Loss: 2.1622, Perplexity: 8.69067\n",
      "Epoch [2/3], Step [8900/12942], Loss: 2.2958, Perplexity: 9.93191\n",
      "Epoch [2/3], Step [9000/12942], Loss: 2.2911, Perplexity: 9.88622\n",
      "Epoch [2/3], Step [9100/12942], Loss: 2.0353, Perplexity: 7.65473\n",
      "Epoch [2/3], Step [9200/12942], Loss: 2.0611, Perplexity: 7.85432\n",
      "Epoch [2/3], Step [9300/12942], Loss: 2.4039, Perplexity: 11.0662\n",
      "Epoch [2/3], Step [9400/12942], Loss: 2.3501, Perplexity: 10.4862\n",
      "Epoch [2/3], Step [9500/12942], Loss: 2.0300, Perplexity: 7.61373\n",
      "Epoch [2/3], Step [9600/12942], Loss: 2.2087, Perplexity: 9.103714\n",
      "Epoch [2/3], Step [9700/12942], Loss: 2.1718, Perplexity: 8.77446\n",
      "Epoch [2/3], Step [9800/12942], Loss: 2.2872, Perplexity: 9.84769\n",
      "Epoch [2/3], Step [9900/12942], Loss: 2.3794, Perplexity: 10.7984\n",
      "Epoch [2/3], Step [10000/12942], Loss: 2.1618, Perplexity: 8.6864\n",
      "Epoch [2/3], Step [10100/12942], Loss: 2.2572, Perplexity: 9.55617\n",
      "Epoch [2/3], Step [10200/12942], Loss: 2.1949, Perplexity: 8.97920\n",
      "Epoch [2/3], Step [10300/12942], Loss: 2.4245, Perplexity: 11.2964\n",
      "Epoch [2/3], Step [10400/12942], Loss: 2.4620, Perplexity: 11.7284\n",
      "Epoch [2/3], Step [10500/12942], Loss: 2.6203, Perplexity: 13.7402\n",
      "Epoch [2/3], Step [10600/12942], Loss: 2.2163, Perplexity: 9.17353\n",
      "Epoch [2/3], Step [10700/12942], Loss: 2.1681, Perplexity: 8.74131\n",
      "Epoch [2/3], Step [10800/12942], Loss: 2.1059, Perplexity: 8.214824\n",
      "Epoch [2/3], Step [10900/12942], Loss: 2.2012, Perplexity: 9.03576\n",
      "Epoch [2/3], Step [11000/12942], Loss: 2.2472, Perplexity: 9.46105\n",
      "Epoch [2/3], Step [11100/12942], Loss: 2.4093, Perplexity: 11.1259\n",
      "Epoch [2/3], Step [11200/12942], Loss: 2.8031, Perplexity: 16.4953\n",
      "Epoch [2/3], Step [11300/12942], Loss: 2.0275, Perplexity: 7.59503\n",
      "Epoch [2/3], Step [11400/12942], Loss: 2.1178, Perplexity: 8.31320\n",
      "Epoch [2/3], Step [11500/12942], Loss: 1.9775, Perplexity: 7.22453\n",
      "Epoch [2/3], Step [11600/12942], Loss: 2.2830, Perplexity: 9.806431\n",
      "Epoch [2/3], Step [11700/12942], Loss: 2.1052, Perplexity: 8.20891\n",
      "Epoch [2/3], Step [11800/12942], Loss: 1.8922, Perplexity: 6.63405\n",
      "Epoch [2/3], Step [11900/12942], Loss: 1.9725, Perplexity: 7.18897\n",
      "Epoch [2/3], Step [12000/12942], Loss: 2.1432, Perplexity: 8.52690\n",
      "Epoch [2/3], Step [12100/12942], Loss: 2.0147, Perplexity: 7.49873\n",
      "Epoch [2/3], Step [12200/12942], Loss: 2.4747, Perplexity: 11.8776\n",
      "Epoch [2/3], Step [12300/12942], Loss: 2.5352, Perplexity: 12.6184\n",
      "Epoch [2/3], Step [12400/12942], Loss: 2.0195, Perplexity: 7.53458\n",
      "Epoch [2/3], Step [12500/12942], Loss: 2.6034, Perplexity: 13.5102\n",
      "Epoch [2/3], Step [12600/12942], Loss: 1.9444, Perplexity: 6.98944\n",
      "Epoch [2/3], Step [12700/12942], Loss: 2.1139, Perplexity: 8.28059\n",
      "Epoch [2/3], Step [12800/12942], Loss: 2.0990, Perplexity: 8.15797\n",
      "Epoch [2/3], Step [12900/12942], Loss: 2.0988, Perplexity: 8.15631\n",
      "Epoch [3/3], Step [100/12942], Loss: 2.1053, Perplexity: 8.2096278\n",
      "Epoch [3/3], Step [200/12942], Loss: 2.1949, Perplexity: 8.97896\n",
      "Epoch [3/3], Step [300/12942], Loss: 2.1737, Perplexity: 8.79091\n",
      "Epoch [3/3], Step [400/12942], Loss: 2.3071, Perplexity: 10.0457\n",
      "Epoch [3/3], Step [500/12942], Loss: 2.2538, Perplexity: 9.52364\n",
      "Epoch [3/3], Step [600/12942], Loss: 1.8424, Perplexity: 6.31189\n",
      "Epoch [3/3], Step [700/12942], Loss: 2.0873, Perplexity: 8.06287\n",
      "Epoch [3/3], Step [800/12942], Loss: 2.4113, Perplexity: 11.1486\n",
      "Epoch [3/3], Step [900/12942], Loss: 5.5273, Perplexity: 251.4689\n",
      "Epoch [3/3], Step [1000/12942], Loss: 2.0370, Perplexity: 7.6675\n",
      "Epoch [3/3], Step [1100/12942], Loss: 2.3203, Perplexity: 10.1792\n",
      "Epoch [3/3], Step [1200/12942], Loss: 2.0940, Perplexity: 8.11705\n",
      "Epoch [3/3], Step [1300/12942], Loss: 2.3428, Perplexity: 10.4099\n",
      "Epoch [3/3], Step [1400/12942], Loss: 2.0441, Perplexity: 7.72237\n",
      "Epoch [3/3], Step [1500/12942], Loss: 1.9715, Perplexity: 7.18156\n",
      "Epoch [3/3], Step [1600/12942], Loss: 2.4182, Perplexity: 11.2260\n",
      "Epoch [3/3], Step [1700/12942], Loss: 2.6541, Perplexity: 14.2121\n",
      "Epoch [3/3], Step [1800/12942], Loss: 2.1908, Perplexity: 8.94209\n",
      "Epoch [3/3], Step [1900/12942], Loss: 2.2104, Perplexity: 9.11964\n",
      "Epoch [3/3], Step [2000/12942], Loss: 2.0372, Perplexity: 7.66897\n",
      "Epoch [3/3], Step [2100/12942], Loss: 2.4127, Perplexity: 11.1639\n",
      "Epoch [3/3], Step [2200/12942], Loss: 2.0176, Perplexity: 7.52025\n",
      "Epoch [3/3], Step [2300/12942], Loss: 1.7338, Perplexity: 5.66216\n",
      "Epoch [3/3], Step [2400/12942], Loss: 2.1732, Perplexity: 8.78620\n",
      "Epoch [3/3], Step [2500/12942], Loss: 2.2255, Perplexity: 9.25777\n",
      "Epoch [3/3], Step [2600/12942], Loss: 1.8246, Perplexity: 6.20050\n",
      "Epoch [3/3], Step [2700/12942], Loss: 2.2206, Perplexity: 9.21261\n",
      "Epoch [3/3], Step [2800/12942], Loss: 1.9606, Perplexity: 7.10382\n",
      "Epoch [3/3], Step [2900/12942], Loss: 2.0975, Perplexity: 8.14579\n",
      "Epoch [3/3], Step [3000/12942], Loss: 2.2641, Perplexity: 9.62249\n",
      "Epoch [3/3], Step [3100/12942], Loss: 2.0523, Perplexity: 7.78608\n",
      "Epoch [3/3], Step [3200/12942], Loss: 1.9067, Perplexity: 6.73096\n",
      "Epoch [3/3], Step [3300/12942], Loss: 1.8704, Perplexity: 6.49086\n",
      "Epoch [3/3], Step [3400/12942], Loss: 2.0680, Perplexity: 7.90875\n",
      "Epoch [3/3], Step [3500/12942], Loss: 2.2866, Perplexity: 9.84187\n",
      "Epoch [3/3], Step [3600/12942], Loss: 2.2805, Perplexity: 9.78162\n",
      "Epoch [3/3], Step [3700/12942], Loss: 2.2575, Perplexity: 9.55880\n",
      "Epoch [3/3], Step [3800/12942], Loss: 1.9754, Perplexity: 7.20973\n",
      "Epoch [3/3], Step [3900/12942], Loss: 2.0915, Perplexity: 8.09706\n",
      "Epoch [3/3], Step [4000/12942], Loss: 2.1736, Perplexity: 8.79008\n",
      "Epoch [3/3], Step [4100/12942], Loss: 2.0788, Perplexity: 7.99483\n",
      "Epoch [3/3], Step [4200/12942], Loss: 1.9826, Perplexity: 7.26180\n",
      "Epoch [3/3], Step [4300/12942], Loss: 2.0035, Perplexity: 7.41471\n",
      "Epoch [3/3], Step [4400/12942], Loss: 2.0857, Perplexity: 8.05044\n",
      "Epoch [3/3], Step [4500/12942], Loss: 2.2106, Perplexity: 9.12126\n",
      "Epoch [3/3], Step [4600/12942], Loss: 1.8488, Perplexity: 6.35244\n",
      "Epoch [3/3], Step [4700/12942], Loss: 2.0827, Perplexity: 8.02592\n",
      "Epoch [3/3], Step [4715/12942], Loss: 2.1656, Perplexity: 8.71976"
     ]
    }
   ],
   "source": [
    "for epoch in range(2, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_value_(decoder.parameters(), clip_value)\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "        # Save the weights.\n",
    "        if i_step % save_every == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "            torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
